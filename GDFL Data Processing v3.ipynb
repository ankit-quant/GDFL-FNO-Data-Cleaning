{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to read and clean the GDFL Data\n",
    "\n",
    "Purpose: Read the GDFL FNO Data CSV file and process the data to save different types of files as below: \n",
    "1. Nifty Options - (Nifty_Date_Options)\n",
    "2. Nifty Futures - (Nifty_Date_Futures)\n",
    "3. BankNifty Options - (BN_Date_Options)\n",
    "4. BankNifty Futures - (BN_Date_Futures)\n",
    "5. Equity Options - (EQ_Date_Options)\n",
    "6. Equity Futures - (EQ_Date_Futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Code \n",
    "\n",
    "1. Read raw CSV File and extract Symbol from the Ticker column\n",
    "2. Assign Index Type as EQFNO or IXFNPO based on the Symbol \n",
    "3. Assign Type = Fut or CE or PE based on Ticker column\n",
    "4. If Index_Type==Nifty \n",
    "        a. If Type==Fut\n",
    "            i. Extract Contract Types  - -I, -II,-III\n",
    "            ii. Write to file - Nifty_Date_Futures\n",
    "        b. Elif Type!=Fut\n",
    "            i. If Date<Nifty Start Date\n",
    "                  Extract Monthly Expiries and Strikes \n",
    "            ii. elif Date>Nifty Start Date\n",
    "                  Extract Weekly Expiries and Strikes\n",
    "            iii. Write to file - Nifty_Date_Options\n",
    "5. Elif Index_Type == BankNifty\n",
    "        a. If Type==Fut\n",
    "            i. Extract Contract Types  - -I, -II,-III\n",
    "            ii. Write to file - BN_Date_Futures\n",
    "        b. Elif Type!=Fut\n",
    "            i. If Date<BankNifty Start Date\n",
    "                  Extract Monthly Expiries and Strikes \n",
    "            ii. elif Date>BankNifty Start Date\n",
    "                  Extract Weekly Expiries and Strikes\n",
    "            iii. Write to file - BN_Date_Options\n",
    "6. Elif Index_Type==EQFNO\n",
    "        a. if Type==Fut\n",
    "            i. Exract Contract Types -> -I, -II, -III\n",
    "            ii. Write to file - EQ_Date_Futures\n",
    "        b. elif Type!=Fut\n",
    "            i. if Date<Nifty Start Date\n",
    "                - Extract Monthly Expiries and Strikes\n",
    "                - Handle exceptions for Decimel Strikes \n",
    "            ii. else\n",
    "                 - Extract Weekly Expiries and Strikes\n",
    "                 - Handle exceptions for Decimel strikes\n",
    "            iii. Write to file - EQ_Date_Options\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nifty Weekly Start Dates\n",
    "nifty_weekly_start=dt.datetime(year=2017,month=2,day=11,hour=0,minute=0,second=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BankNifty Weekly Start Dates\n",
    "bn_weekly_start=dt.datetime(year=2016,month=5,day=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter the Directory where the final pkl files are to be stored\n",
    "save_directory=r'C:\\Users\\ankit\\Documents\\Jupyter Notebooks\\Systematic Trading - BackTests\\Data Pickles\\GDFL Clean Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter the source data Directory from where to read data\n",
    "src_directory=r'C:\\Users\\ankit\\Dropbox\\Ventures\\Trading\\NSE Historical Data\\Options Datamart Code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to read String format of Date and Time columns and create a unique DateTime type\n",
    "## Inputs - DataFrame of Raw Data\n",
    "## Outputs - Retruns Column with DateTime type\n",
    "\n",
    "def extract_datetime(df):\n",
    "    df['DateTime']=df['Date']+' '+df['Time']\n",
    "    df['DateTime']=df.DateTime.apply(lambda x:dt.datetime.strptime(x,'%d/%m/%Y %H:%M:%S'))\n",
    "    print('Cleaning Data for:'+df['Date'][0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to read String format of Ticker columns and create two columns \n",
    "## - Column Type - has values as Fut, CE or PE\n",
    "## - Column Index - has values IXFNO or EQFNO \n",
    "## Inputs - DataFrame of Data\n",
    "## Outputs - Retuns 2 columns \n",
    "## - data['Type'] and data['Index']\n",
    "\n",
    "def extract_indextype(df):\n",
    "    df['Ticker']=df['Ticker'].str.upper()\n",
    "    \n",
    "    ## Split df to pick up CE and PE as Type from the Ticker Name\n",
    "    df['Type']=df['Ticker'].apply(lambda x:x.split('.NFO')[0][-2:])\n",
    "    \n",
    "    ## Replace -I, II,III with Fut\n",
    "    df['Type']=np.where((df['Type']!='CE')&(df['Type']!='PE'),'Fut',df['Type'])\n",
    "    \n",
    "    ## All Index Tickers have NIFTY in it - NIFTY, BANKNIFTY, NIFTYIT, NIFTYINFRA, MINIFTY\n",
    "    df['Index']=df['Ticker'].apply(lambda x:'IXFNO' if 'NIFTY' in x else 'EQFNO')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symbol(df):\n",
    "    # Read the Ticker and identify the Symbol Names from it\n",
    "    # This uses REGEX within str.extract() method to identify the pattern\n",
    "    # For Type_name==OPT, fomat is as above noted\n",
    "    # \\w matches AlphaNumeric characters, followed by '+' matches multiple length\n",
    "    # [0-9] matches a digit or can also use \\d \n",
    "    # To Match futures - ACC-I; \"-\" is used as an escaping character\n",
    "    df['Symbol']=np.where(df['Type']!='Fut',\\\n",
    "                        df['Ticker'].str.extract(r'([A-Za-z0-9-&]+[0-9][0-9][A-Z][A-Z][A-Z])',expand=True)[0].astype(str).\\\n",
    "                        apply(lambda x:x[:-5]),\n",
    "                        df['Ticker'].str.extract(r'([A-Za-z0-9-&]+\\-)',expand=True)[0].astype(str).apply(lambda x:x[:-1]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Future Types (I Month, II Second Month, III Month)\n",
    "def extract_contract(df):\n",
    "    df['Contract']=df['Ticker'].apply(lambda x:x.split('-')[-1].split('.')[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weekly(df):\n",
    "    ## Example: NIFTY24JUN2110600CE.NFO\n",
    "    df['Expiry']=pd.to_datetime(\n",
    "                    df['Ticker'].str.extract(r'([0-9][0-9][A-Z][A-Z][A-Z]\\d+[A-Z][A-Z])')[0].\\\n",
    "                    astype(str).apply(lambda x:x[:7]),\\\n",
    "                    format='%d%b%y')\n",
    "    df['Strike']=df['Ticker'].str.extract(r'([0-9][0-9][A-Z][A-Z][A-Z]\\d+[A-Z][A-Z])')[0].astype(str).\\\n",
    "                        apply(lambda x:x[7:-2])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_monthly(df):\n",
    "    ## Example: NIFTY16JUN10600CE.NFO\n",
    "    df['Expiry']=pd.to_datetime(\n",
    "                    df['Ticker'].str.extract(r'(\\d\\d[A-Z][A-Z][A-Z]\\d+[A-Z][A-Z])')[0].astype(str).apply(lambda x:x[:5]),\\\n",
    "                    format='%y%b')\n",
    "    df['Strike']=df['Ticker'].str.extract(r'(\\d\\d[A-Z][A-Z][A-Z]\\d+[A-Z][A-Z])')[0].astype(str).apply(lambda x:x[5:-2])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a slice of main data for Nifty futures and apply appropriate features \n",
    "## Save the cleaned slice of data as a separate pickle file\n",
    "def clean_nifty_future(df):\n",
    "    nifty_fut=df[(df['Symbol']=='NIFTY') & (df['Type']=='Fut')]\n",
    "    nifty_fut.reset_index(inplace=True,drop=True)\n",
    "    extract_contract(nifty_fut)\n",
    "    date=nifty_fut['DateTime'][0].strftime(format='%Y-%m-%d')\n",
    "    filename='nifty_%s_fut.pkl'%(date)\n",
    "    nifty_fut.to_pickle(save_directory+'\\\\'+filename)\n",
    "    #print('Saved Nifty Future for:'+date)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a slice of main data for Nifty Options and create appropriate expiry and strike columns\n",
    "## Save the data in appropriate files\n",
    "def clean_nifty_opt(df):\n",
    "    nifty_opt=data[(data['Symbol']=='NIFTY') & (data['Type']!='Fut')]\n",
    "    nifty_opt.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    if (nifty_opt['DateTime'][0]>=nifty_weekly_start):\n",
    "        extract_weekly(nifty_opt)\n",
    "    else:\n",
    "        extract_monthly(nifty_opt)\n",
    "    date=nifty_opt['DateTime'][0].strftime(format='%Y-%m-%d')\n",
    "    filename='nifty_%s_opt.pkl'%(date)\n",
    "    nifty_opt.to_pickle(save_directory+'\\\\'+filename)\n",
    "    #print('Saved Nifty Options for:'+date)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a slice of main data for BanNifty futures and apply appropriate features \n",
    "## Save the cleaned slice of data as a separate pickle file\n",
    "def clean_bn_future(df):\n",
    "    bn_fut=df[(df['Symbol']=='BANKNIFTY') & (df['Type']=='Fut')]\n",
    "    bn_fut.reset_index(inplace=True,drop=True)\n",
    "    extract_contract(bn_fut)\n",
    "    date=bn_fut['DateTime'][0].strftime(format='%Y-%m-%d')\n",
    "    filename='bn_%s_fut.pkl'%(date)\n",
    "    bn_fut.to_pickle(save_directory+'\\\\'+filename)\n",
    "    #print('Saved BankNifty Future for:'+date)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a slice of main data for Nifty Options and create appropriate expiry and strike columns\n",
    "## Save the data in appropriate files\n",
    "def clean_bn_opt(df):\n",
    "    bn_opt=data[(data['Symbol']=='BANKNIFTY') & (data['Type']!='Fut')]\n",
    "    bn_opt.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    if (bn_opt['DateTime'][0]>=bn_weekly_start):\n",
    "        extract_weekly(bn_opt)\n",
    "    else:\n",
    "        extract_monthly(bn_opt)\n",
    "    date=bn_opt['DateTime'][0].strftime(format='%Y-%m-%d')\n",
    "    filename='bn_%s_opt.pkl'%(date)\n",
    "    bn_opt.to_pickle(save_directory+'\\\\'+filename)\n",
    "    #print('Saved BankNifty Options for:'+date)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a slice of main data that does not match Nifty or BanNifty futures and apply appropriate features \n",
    "## Save the cleaned slice of data as a separate pickle file\n",
    "def clean_eq_future(df):\n",
    "    eq_fut=data[(data['Symbol']!='BANKNIFTY')&(data['Symbol']!='NIFTY')&(data['Type']=='Fut')]\n",
    "    eq_fut.reset_index(inplace=True,drop=True)\n",
    "    extract_contract(eq_fut)\n",
    "    date=eq_fut['DateTime'][0].strftime(format='%Y-%m-%d')\n",
    "    filename='eq_%s_fut.pkl'%(date)\n",
    "    eq_fut.to_pickle(save_directory+'\\\\'+filename)\n",
    "    #print('Saved EQ Future for:'+date)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stocks may have decimal strikes and those need to be addressed\n",
    "## The function below intakes a dataframe and date (weekly start date)\n",
    "## it extracts decimal strikes and then convert expiries to datetime\n",
    "## previous expiries were converted for non-decimal strikes and thus needs special treatment in code\n",
    "def extract_decimal(df,date):\n",
    "    if(date>=nifty_weekly_start):\n",
    "        df['Expiry']=np.where(df['Expiry'].isnull(),\\\n",
    "                                    df['Ticker'].str.extract(r'([0-9][0-9][A-Z][A-Z][A-Z]\\d+\\.\\d+[A-Z][A-Z])')[0].\\\n",
    "                                        astype(str).apply(lambda x:x[:7]),\\\n",
    "                                df.Expiry)\n",
    "        \n",
    "        ## The above code results in a mixed column of expiry data types\n",
    "        # First type is date of format : 23FEB19\n",
    "        date1=pd.to_datetime(df['Expiry'],format='%d%b%y',errors='coerce')\n",
    "        # Second type is previously converted dates with timestamps in nanoseconds from epoch\n",
    "        date2=pd.to_datetime(df['Expiry'],unit='ns',errors='coerce')\n",
    "        #after converting both types to datetime, use fillna to combine them together\n",
    "        df['Expiry']=date1.fillna(date2)\n",
    "        \n",
    "        df['Strike']=np.where(df['Strike']=='',\\\n",
    "                                  df['Ticker'].str.extract(r'([0-9][0-9][A-Z][A-Z][A-Z]\\d+\\.\\d+[A-Z][A-Z])')[0].\\\n",
    "                                  astype(str).apply(lambda x:x[7:-2]),\\\n",
    "                              df.Strike)\n",
    "    else:\n",
    "        df['Expiry']=np.where(df['Expiry'].isnull(),\\\n",
    "                                  df['Ticker'].str.extract(r'([0-9][0-9][A-Z][A-Z][A-Z]\\d+\\.\\d+[A-Z][A-Z])')[0].\\\n",
    "                                  astype(str).apply(lambda x:x[:5]),\\\n",
    "                              df.Expiry)\n",
    "        ## The above code results in a mixed column of expiry data types\n",
    "        # First type is date of format : 16FEB\n",
    "        date1=pd.to_datetime(df['Expiry'],format='%y%b',errors='coerce')\n",
    "        # Second type is previously converted dates with timestamps in nanoseconds from epoch\n",
    "        date2=pd.to_datetime(df['Expiry'],unit='ns',errors='coerce')\n",
    "        #after converting both types to datetime, use fillna to combine them together\n",
    "        df['Expiry']=date1.fillna(date2)\n",
    "        \n",
    "        df['Strike']=np.where(df['Strike']=='',\\\n",
    "                                  df['Ticker'].str.extract(r'([0-9][0-9][A-Z][A-Z][A-Z]\\d+\\.\\d+[A-Z][A-Z])')[0].\\\n",
    "                                  astype(str).apply(lambda x:x[5:-2]),\\\n",
    "                              df.Strike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a slice of main data that does not match Nifty or BanNifty options and apply appropriate features \n",
    "## Save the cleaned slice of data as a separate pickle file\n",
    "def clean_eq_opt(df):\n",
    "    eq_opt=data[(data['Symbol']!='BANKNIFTY')&(data['Symbol']!='NIFTY')&(data['Type']!='Fut')]\n",
    "    eq_opt.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    if(eq_opt['DateTime'][0]>=nifty_weekly_start):\n",
    "        extract_weekly(eq_opt)\n",
    "    else:\n",
    "        extract_monthly(eq_opt)\n",
    "    ## Stock options may have decimal strikes and needs to be cleaned up\n",
    "    extract_decimal(eq_opt,eq_opt['DateTime'][0])\n",
    "    date=eq_opt['DateTime'][0].strftime(format='%Y-%m-%d')\n",
    "    filename='eq_%s_opt.pkl'%(date)\n",
    "    eq_opt.to_pickle(save_directory+'\\\\'+filename)\n",
    "    #print('Saved EQ Option for:'+date)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    extract_datetime(df)\n",
    "    extract_indextype(df)\n",
    "    extract_symbol(df)\n",
    "    clean_nifty_future(df)\n",
    "    clean_nifty_opt(df)\n",
    "    clean_bn_future(df)\n",
    "    clean_bn_opt(df)\n",
    "    clean_eq_future(df)\n",
    "    clean_eq_opt(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This functions taken two inputs\n",
    "def new_date_check(dates_list,search_date):    \n",
    "    flag=True\n",
    "    for d in dates_list:\n",
    "        if d==search_date:\n",
    "            flag=False\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Data For: 2019-02-01 00:00:00\n",
      "Cleaning Data for:1/2/2019\n",
      "Cleaning Data For: 2021-03-12 00:00:00\n",
      "Cleaning Data for:12/3/2021\n",
      "Cleaning Data For: 2011-04-01 00:00:00\n",
      "Cleaning Data for:1/4/2011\n",
      "Cleaning Data For: 2016-01-15 00:00:00\n",
      "Cleaning Data for:15/01/2016\n",
      "Cleaning Data For: 2016-06-16 00:00:00\n",
      "Cleaning Data for:16/06/2016\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory and save it in a variable. This will be used to switch back after we have read data\n",
    "current_directory=os.getcwd()\n",
    "\n",
    "#Change the working directory to the path provided above. \n",
    "os.chdir(src_directory)\n",
    "\n",
    "# Read all pkl files stores in the directory where clean data is stored \n",
    "clean_data_files=os.listdir(save_directory)\n",
    "\n",
    "# Extract the list of unique dates for which we already have the data\n",
    "dates_list=[dt.datetime.strptime(str(x.split('_')[-2]),'%Y-%m-%d') for x in clean_data_files]\n",
    "dates_list=np.unique(dates_list)\n",
    "\n",
    "#Run a Loop that lists total no of files within the directory path. \n",
    "#It is important that there are only readable data in the same folder\n",
    "\n",
    "\n",
    "for file1 in os.listdir(src_directory):\n",
    "    \n",
    "    #read the csv into an empty new variable - df\n",
    "    data=pd.read_csv(file1,sep=',')\n",
    "    \n",
    "    search_date=dt.datetime.strptime(data['Date'][0],'%d/%m/%Y')\n",
    "    \n",
    "    if(new_date_check(dates_list,search_date)):\n",
    "        print('Cleaning Data For: '+str(search_date))\n",
    "        clean_data(data)\n",
    "    else:\n",
    "        print('Clean Data already exists for: '+str(search_date)+'.....Reading next file')\n",
    "        continue\n",
    "        \n",
    "#Revert the working directory to the original directory from wher we started\n",
    "os.chdir(current_directory)\n",
    "\n",
    "#os.listdir(src_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
